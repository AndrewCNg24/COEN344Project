{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading training dataset MS1M-ArcFace (85K ids/5.8M images) and unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1SXS4-Am3bsKSK615qbYdbA_FMVh3sAvR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1SXS4-Am3bsKSK615qbYdbA_FMVh3sAvR\" -O faces_emore.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip faces_emore.zip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get evaluation data and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1HBGwyTFnl4Bt4hl5BpLE3t__J84R72TX' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1HBGwyTFnl4Bt4hl5BpLE3t__J84R72TX\" -O faces_emore.zip && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar MagFace-main/eval/eval_recognition -xvf lfw_cfp_agedb.tar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract data training data to /dataset/data (stop when needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python python rec2image.py --include ./dataset/faces_emore --output ./dataset/data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install depenencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies needed for Colab\n",
    "colab_Dependencies = [\"torchshard\", \"loguru\", \"sklearn\", \"logger\", \"mxnet-cu101\", \"mxnet\"]\n",
    "for lib in colab_Dependencies:\n",
    "  print(f\"Installing {lib}\\n\")\n",
    "  %pip install {lib}\n",
    "  print(f\"\\n{lib} installed!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies \n",
    "\n",
    "requirments_list = [\"torchvision\", \"torchshard\", \"torch\", \"numpy\", \"scipy\", \"termcolor\", \"opencv-contrib-python\", \"jupyterlab\", \"notebook\", \"seaborn\", \"Pillow==6.2.2\", \"matplotlib \", \"loguru\", \"six\", \"imageio\", \"scikit-image\", \"tqdm\", \"sklearn\"]\n",
    "for lib in requirments_list:\n",
    "  print(f\"Installing {lib}\\n\")\n",
    "  %pip install {lib}\n",
    "  print(f\"\\n{lib} installed!\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Align faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python face_align.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dataset_Path = '/datasets/data'\n",
    "dataset_Folders = os.listdir(dataset_Path)\n",
    "print(sorted(dataset_Folders))\n",
    "# list of values to remove\n",
    "remove_list = ['faces_emore', 'train.list', '.train.list.swp']\n",
    "for rm in remove_list:\n",
    "  if rm in dataset_Folders:\n",
    "    dataset_Folders.remove(rm)\n",
    "print(sorted(dataset_Folders))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, remove empty folders from training set (Case where image extraction ended early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of id's\n",
    "# Remove id's with no images\n",
    "print(\"Folders with 0 files\")\n",
    "for folder in dataset_Folders:\n",
    "  length = len(os.listdir(f\"{dataset_Path}/{folder}\"))\n",
    "  if length == 0:\n",
    "    print(f\"Removing {folder}\")\n",
    "    path = f\"{dataset_Path}/{folder}\"\n",
    "    %rmdir {path}\n",
    "# Confirm list\n",
    "emptyFolderList = []\n",
    "dataset_Folders = os.listdir(dataset_Path)\n",
    "# Remove permenent folder / files\n",
    "for rm in remove_list:\n",
    "  if rm in dataset_Folders:\n",
    "    dataset_Folders.remove(rm)\n",
    "\n",
    "for folder in dataset_Folders:\n",
    "  length = len(os.listdir(f\"{dataset_Path}/{folder}\"))\n",
    "  if length == 0:\n",
    "    emptyFolderList.append(folder)\n",
    "\n",
    "if len(emptyFolderList) == 0:\n",
    "  print(\"No empty folders\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a train.list file to train with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write list to train.list\n",
    "dataset_Folders = os.listdir(dataset_Path)\n",
    "# Create train.list file\n",
    "f = open(f\"{dataset_Path}/train.list\", \"w\")\n",
    "# Write list\n",
    "for idx, folder in enumerate(dataset_Folders):\n",
    "  folder_Path = (f\"{dataset_Path}/{folder}\")\n",
    "  img_List = os.listdir(f\"{folder_Path}\")\n",
    "  for img in img_List:\n",
    "    f.write(f\"{folder_Path}/{img} 0 {idx} 0\\n\")\n",
    "\n",
    "f.close()\n",
    "\n",
    "dataset_Path = '/content/drive/MyDrive/datasets'\n",
    "#open and read the file after the appending:\n",
    "f = open(f\"{dataset_Path}/train.list\", \"r\")\n",
    "# Checking for number of images\n",
    "x = len(f.readlines())\n",
    "print('Total lines:', x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for default values\n",
    "%%shell\n",
    "cd /run\n",
    "bash ./run_dist.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with gaussian filter\n",
    "%%shell\n",
    "cd /run\n",
    "bash ./run_dist.sh --gaus True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using WAVE adjust the \"job-name\" and \"mail-user\" values in the \"./run/run_dist.sh\" file, use the following commands in the terminal:\n",
    "\n",
    "ssh login1 (fill in your)\n",
    "\n",
    "module load PyTorch/1.12.1\n",
    "\n",
    "module load TensorFlow/2.11.0-20230208\n",
    "\n",
    "cd /run\n",
    "\n",
    "<!-- Default training -->\n",
    "\n",
    "sbatch -p gpu -N1 -n1 -c2 --gres=gpu:1 --time=08:00:00 --mem=32G run_dist.sh\n",
    "\n",
    "<!-- with gaussian fitler -->\n",
    "sbatch -p gpu -N1 -n1 -c2 --gres=gpu:1 --time=08:00:00 --mem=32G run_dist.sh --gaus True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to evalute the trained model should output accuracy ratings for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%shell\n",
    "cd /eval/eval_recognition\n",
    "./eval.sh /run/test/00025.pth official 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using WAVE\n",
    "\n",
    "ssh login1 (fill in your)\n",
    "\n",
    "module load PyTorch/1.12.1\n",
    "\n",
    "module load TensorFlow/2.11.0-20230208\n",
    "\n",
    "cd /eval/eval_recognition\n",
    "\n",
    "sbatch -p gpu -N1 -n1 -c2 --gres=gpu:1 --time=08:00:00 --mem=32G ./eval.sh /run/test/00025.pth official 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeOutput(values):\n",
    "  epoch0, epoch1, epoch2, epoch3, epoch4, epoch5, epoch6 = [], [], [], [], [], [], []\n",
    "  epochs = [epoch0, epoch1, epoch2, epoch3, epoch4, epoch5, epoch6]\n",
    "  epochCounter = 0\n",
    "  for epochCounter in range(7):\n",
    "    for idx, line in enumerate(values):\n",
    "      if (f'[{epochCounter}]') in line:\n",
    "        # print(line)\n",
    "        # Epoch 0\n",
    "        if epochCounter == 0:\n",
    "          epoch0.append(line)\n",
    "        # Epoch 1\n",
    "        if epochCounter == 1:\n",
    "          epoch1.append(line)\n",
    "        # Epoch 2\n",
    "        if epochCounter == 2:\n",
    "          epoch2.append(line)\n",
    "        # Epoch 3\n",
    "        if epochCounter == 3:\n",
    "          epoch3.append(line)\n",
    "        # Epoch 4\n",
    "        if epochCounter == 4:\n",
    "          epoch4.append(line)\n",
    "        # Epoch 5\n",
    "        if epochCounter == 5:\n",
    "          epoch5.append(line)\n",
    "        # Epoch 6\n",
    "        if epochCounter == 6:\n",
    "          epoch6.append(line)\n",
    "        # Epoch 7\n",
    "        if epochCounter == 7:\n",
    "          epoch6.append(line)\n",
    "  return(epochs)\n",
    "  print('Epochs separeated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValues(epochs):\n",
    "  # Loss\n",
    "  ep0Loss, ep1Loss, ep2Loss, ep3Loss, ep4Loss, ep5Loss, ep6Loss = [], [], [], [], [], [], []\n",
    "  epochLoss = [ep0Loss, ep1Loss, ep2Loss, ep3Loss, ep4Loss, ep5Loss, ep6Loss]\n",
    "  # Accuracy\n",
    "  ep0Acc, ep1Acc, ep2Acc, ep3Acc, ep4Acc, ep5Acc, ep6Acc = [], [], [], [], [], [], []\n",
    "  epochAcc = [ep0Acc, ep1Acc, ep2Acc, ep3Acc, ep4Acc, ep5Acc, ep6Acc]\n",
    "  # Normal\n",
    "  ep0NormMean, ep1NormMean, ep2NormMean, ep3NormMean, ep4NormMean, ep5NormMean, ep6NormMean = [], [], [], [], [], [], []\n",
    "  ep0NormMeanMin, ep1NormMeanMin, ep2NormMeanMin, ep3NormMeanMin, ep4NormMeanMin, ep5NormMeanMin, ep6NormMeanMin = [], [], [], [], [], [], []\n",
    "  ep0NormMeanMax, ep1NormMeanMax, ep2NormMeanMax, ep3NormMeanMax, ep4NormMeanMax, ep5NormMeanMax, ep6NormMeanMax = [], [], [], [], [], [], []\n",
    "  epochNormal = [ep0NormMean, ep1NormMean, ep2NormMean, ep3NormMean, ep4NormMean, ep5NormMean, ep6NormMean]\n",
    "  epochNormalMin = [ep0NormMeanMin, ep1NormMeanMin, ep2NormMeanMin, ep3NormMeanMin, ep4NormMeanMin, ep5NormMeanMin, ep6NormMeanMin]\n",
    "  epochNormalMax = [ep0NormMeanMax, ep1NormMeanMax, ep2NormMeanMax, ep3NormMeanMax, ep4NormMeanMax, ep5NormMeanMax, ep6NormMeanMax]\n",
    "  # Margin\n",
    "  ep0MargMean, ep1MargMean, ep2MargMean, ep3MargMean, ep4MargMean, ep5MargMean, ep6MargMean = [], [], [], [], [], [], []\n",
    "  ep0MargMeanMin, ep1MargMeanMin, ep2MargMeanMin, ep3MargMeanMin, ep4MargMeanMin, ep5MargMeanMin, ep6MargMeanMin = [], [], [], [], [], [], []\n",
    "  ep0MargMeanMax, ep1MargMeanMax, ep2MargMeanMax, ep3MargMeanMax, ep4MargMeanMax, ep5MargMeanMax, ep6MargMeanMax = [], [], [], [], [], [], []\n",
    "  epochMargin = [ep0MargMean, ep1MargMean, ep2MargMean, ep3MargMean, ep4MargMean, ep5MargMean, ep6MargMean]\n",
    "  epochMarginMin = [ep0MargMeanMin, ep1MargMeanMin, ep2MargMeanMin, ep3MargMeanMin, ep4MargMeanMin, ep5MargMeanMin, ep6MargMeanMin]\n",
    "  epochMarginMax = [ep0MargMeanMax, ep1MargMeanMax, ep2MargMeanMax, ep3MargMeanMax, ep4MargMeanMax, ep5MargMeanMax, ep6MargMeanMax]\n",
    "\n",
    "  for ep in epochs:\n",
    "    for line in ep:\n",
    "      if 'Epoch' in line:\n",
    "        words = line.split('\\t')\n",
    "        firstLine = words[0].split(' ')\n",
    "        # Epoch\n",
    "        epochText = firstLine[12].replace(']', ' ')\n",
    "        epochText = epochText.replace('[', ' ')\n",
    "        epochText = epochText.split(\" \")\n",
    "        epVal = int(epochText[1])\n",
    "        # Loss\n",
    "        lossText = words[5].split(\" \")\n",
    "        loss = float(lossText[1])\n",
    "        # Accuracy\n",
    "        accText = words[8].split(\" \")\n",
    "        if accText[2] != '':\n",
    "          acc = float(accText[2])\n",
    "        else:\n",
    "          acc = float(accText[3])\n",
    "        # Epoch 0\n",
    "        if epVal == 0:\n",
    "          ep0Loss.append(loss)\n",
    "          ep0Acc.append(acc)\n",
    "        # Epoch 1\n",
    "        if epVal == 1:\n",
    "          ep1Loss.append(loss)\n",
    "          ep1Acc.append(acc)\n",
    "        # Epoch 2\n",
    "        if epVal == 2:\n",
    "          ep2Loss.append(loss)\n",
    "          ep2Acc.append(acc)\n",
    "        # Epoch 3\n",
    "        if epVal == 3:\n",
    "          ep3Loss.append(loss)\n",
    "          ep3Acc.append(acc)\n",
    "        # Epoch 4\n",
    "        if epVal == 4:\n",
    "          ep4Loss.append(loss)\n",
    "          ep4Acc.append(acc)\n",
    "        # Epoch 5\n",
    "        if epVal == 5:\n",
    "          ep5Loss.append(loss)\n",
    "          ep5Acc.append(acc)\n",
    "        # Epoch 6\n",
    "        if epVal == 6:\n",
    "          ep6Loss.append(loss)\n",
    "          ep6Acc.append(acc)\n",
    "      \n",
    "          \n",
    "      if 'debug info' in line:\n",
    "        words = line.split(' ')\n",
    "        debugEp = words[2]\n",
    "        debugEp = debugEp.replace(']', '')\n",
    "        debugEp = debugEp.replace('[', '')\n",
    "        # for idx, i in enumerate(words):\n",
    "        #   print(f\"{idx}: {i}\")\n",
    "        if (('x_norm' in line)) and (debugEp == '0'):\n",
    "          ep0NormMean.append(float(words[7]))\n",
    "          ep0NormMeanMin.append(float(words[9]))\n",
    "          ep0NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '1'):\n",
    "          ep1NormMean.append(float(words[7]))\n",
    "          ep1NormMeanMin.append(float(words[9]))\n",
    "          ep1NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '2'):\n",
    "          ep2NormMean.append(float(words[7]))\n",
    "          ep2NormMeanMin.append(float(words[9]))\n",
    "          ep2NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '3'):\n",
    "          ep3NormMean.append(float(words[7]))\n",
    "          ep3NormMeanMin.append(float(words[9]))\n",
    "          ep3NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '4'):\n",
    "          ep4NormMean.append(float(words[7]))\n",
    "          ep4NormMeanMin.append(float(words[9]))\n",
    "          ep4NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '5'):\n",
    "          ep5NormMean.append(float(words[7]))\n",
    "          ep5NormMeanMin.append(float(words[9]))\n",
    "          ep5NormMeanMax.append(float(words[11]))\n",
    "        if ('x_norm' in line) and (debugEp == '6'):\n",
    "          ep6NormMean.append(float(words[7]))\n",
    "          ep6NormMeanMin.append(float(words[9]))\n",
    "          ep6NormMeanMax.append(float(words[11]))\n",
    "\n",
    "        if 'margin'in line and (debugEp == '0'):\n",
    "          ep0MargMean.append(float(words[7]))\n",
    "          ep0MargMeanMin.append(float(words[9]))\n",
    "          ep0MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '1'):\n",
    "          ep1MargMean.append(float(words[7]))\n",
    "          ep1MargMeanMin.append(float(words[9]))\n",
    "          ep1MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '2'):\n",
    "          ep2MargMean.append(float(words[7]))\n",
    "          ep2MargMeanMin.append(float(words[9]))\n",
    "          ep2MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '3'):\n",
    "          ep3MargMean.append(float(words[7]))\n",
    "          ep3MargMeanMin.append(float(words[9]))\n",
    "          ep3MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '4'):\n",
    "          ep4MargMean.append(float(words[7]))\n",
    "          ep4MargMeanMin.append(float(words[9]))\n",
    "          ep4MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '5'):\n",
    "          ep5MargMean.append(float(words[7]))\n",
    "          ep5MargMeanMin.append(float(words[9]))\n",
    "          ep5MargMeanMax.append(float(words[11]))\n",
    "        if 'margin'in line and (debugEp == '6'):\n",
    "          ep6MargMean.append(float(words[7]))\n",
    "          ep6MargMeanMin.append(float(words[9]))\n",
    "          ep6MargMeanMax.append(float(words[11]))\n",
    "  print(\"Complete\")\n",
    "  return(epochLoss, epochAcc, epochNormal, epochNormalMin, epochNormalMax, epochMargin, epochMarginMin, epochMarginMax)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotValues(loss, acc, norm, normMin, normMax, marg, margMin, margMax, label):\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  for idx in range(7):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = [14, 4]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(loss[idx])\n",
    "    plt.title(f\"Epoch {idx} Loss ({label})\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(acc[idx])\n",
    "    plt.title(f\"Epoch {idx} Acc ({label})\")\n",
    "\n",
    "    plt.savefig(f'/output/{label}/lossAccEp{idx}.png')\n",
    "    plt.show()\n",
    "\n",
    "  for idx in range(7):\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = [14, 4]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.plot(norm[idx], label=\"Normal\")\n",
    "    plt.plot(normMin[idx], label=\"Min\")\n",
    "    plt.plot(normMax[idx], label=\"Max\")\n",
    "    plt.title(f\"Epoch {idx} Norm ({label})\")\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.plot(marg[idx], label=\"Margin\")\n",
    "    plt.plot(margMin[idx], label=\"Min\")\n",
    "    plt.plot(margMax[idx], label=\"Max\")\n",
    "    plt.title(f\"Epoch {idx} Marg ({label})\")\n",
    "\n",
    "    plt.savefig(f'/output/{label}/normMarEp{idx}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basePath = ['run/test/output.log', 'Base']\n",
    "outputs = [basePath]\n",
    "for outF, label in outputs:\n",
    "  print(label)\n",
    "  with open(outF) as my_file:\n",
    "      outputLog = my_file.readlines()\n",
    "  epochs = storeOutput(outputLog)\n",
    "  loss, acc, norm, normMin, normMax, marg, margMin, margMax = getValues(epochs)\n",
    "  plotValues(loss, acc, norm, normMin, normMax, marg, margMin, margMax, label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph loss and accuracy over all epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, acc, norm, normMin, normMax, marg, margMin, margMax = getValues(epochs)\n",
    "import matplotlib.pyplot as plt\n",
    "L, A = [], []\n",
    "fullLoss = [L, loss]\n",
    "fullAcc = [A, acc]\n",
    "full = [fullLoss, fullAcc]\n",
    "\n",
    "for i in full:\n",
    "  for ep in i[1]:\n",
    "    for val in ep:\n",
    "      i[0].append(val)\n",
    "\n",
    "plt.plot(L)\n",
    "plt.xlabel('1 per 100 Batchs (size 256)')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss')\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "# plotting the points \n",
    "plt.plot(A)\n",
    "plt.xlabel('1 per 100 Batchs (size 256)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "  \n",
    "# function to show the plot\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphing normals and margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No, NMin, NMax= [], [], []\n",
    "fullNorm = [No, norm]\n",
    "fullNormMin = [NMin, normMin]\n",
    "fullNormMax = [NMax, normMax]\n",
    "fullNorms = [fullNorm, fullNormMin, fullNormMax]\n",
    "\n",
    "for i in fullNorms:\n",
    "  for ep in i[1]:\n",
    "    for val in ep:\n",
    "      i[0].append(val)\n",
    "\n",
    "# plotting the points \n",
    "plt.plot(No)\n",
    "plt.plot(NMin)\n",
    "plt.plot(NMax)\n",
    "plt.xlabel('1 per 100 Batchs (size 256)')\n",
    "plt.ylabel('Norm')\n",
    "# giving a title to my graph\n",
    "plt.title('Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ma, MMin, MMax= [], [], []\n",
    "fullMarg = [Ma, marg]\n",
    "fullMargMin = [MMin, margMin]\n",
    "fullMargMax = [MMax, margMax]\n",
    "fullMargs = [fullMarg, fullMargMin, fullMargMax]\n",
    "\n",
    "for i in fullMargs:\n",
    "  for ep in i[1]:\n",
    "    for val in ep:\n",
    "      i[0].append(val)\n",
    "\n",
    "# plotting the points \n",
    "plt.plot(Ma)\n",
    "plt.plot(MMin)\n",
    "plt.plot(MMax)\n",
    "plt.xlabel('1 per 100 Batchs (size 256)')\n",
    "plt.ylabel('Marg')\n",
    "# giving a title to my graph\n",
    "plt.title('Margin')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
